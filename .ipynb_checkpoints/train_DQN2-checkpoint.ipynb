{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Connect4 with a Deep-Q-Learning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background:\n",
    "\n",
    "- **Q-learning** is a model-free reinforcement learning algorithm that learns the quality of actions to take under certain circumstances. The algorithm focuses on optimizing the value function according to the environment or problem. The Q in Q-learning represents the quality with which the model finds its next action, improving the quality. The Q-Table helps to find the best action for each state. Q-learning is advantageous because it is model-free, can optimize to get the best possible result without being strictly tethered to a policy, is flexible, and can be trained offline.\n",
    "\n",
    "- **Deep Q-Learning** is a variant of Q-Learning that uses a deep neural network to represent the Q-function. This allows the algorithm to handle environments with a large number of states and actions, as well as to learn from high-dimensional inputs such as images or sensor data. The deep Q-learning model combines Q-learning and a neural network to find the optimal Q-value function. The algorithm finds the input and the optimal Q-value for all possible actions as the output.\n",
    "\n",
    "Where **s** denotes the current state of the game, **a** denotes the current action and **s'** the next state of the game, the reinforcement learning algorithm uses Bellman's Equation to update the agents policy:\n",
    "\n",
    "1. Create units of \"experience\" **(s,a,s',reward)** through play\n",
    "\n",
    "<center><img src=\"data/c4_memory_unit.PNG\" width=500 height=500 /></center>\n",
    "\n",
    "2. Update Q-Values for the state via Bellman's equation\n",
    "\n",
    "<center><img src=\"data/c4_bellman.PNG\" width=500 height=500 /></center>\n",
    "\n",
    "3. Train Deep-Q Network (the image depicts the network architecture and the early stopping criteria used)\n",
    "\n",
    "<center><img src=\"data/c4_train.PNG\" width=500 height=500 /></center>\n",
    "\n",
    "... and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizing Opponents Experience\n",
    "\n",
    "I had the idea to capture the opponents point of view as experience of my own (i.e. the agent). This should be possible in games with full information. I think it greatly accelerates learning. Without this method it can take a long time for the agent to experience wins against stronger agents.\n",
    "\n",
    "<center><img src=\"data/c4_memory_translate.PNG\" width=500 height=500 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Protocol (loosely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st Training\n",
    "- Agent first learned to play against Negamax (negamax agent provided by kaggle)\n",
    "- Retrained after every 32 games (total of 32000 games)\n",
    "- Memory cleared after every 32 games (difficulty processing large batches >100 games; as agent improves, past experience is less valuable?!)\n",
    "- Epsilon greedy policy applied to each move (epsilon = 10%)\n",
    "- Avg. Reward over 32 games plotted\n",
    "\n",
    "<center><img src=\"data/c4_negamax_train.png\" width=500 height=500 /></center>\n",
    "\n",
    "- Note that the agent dominates Negamax after this training. The epsilon greedy policy drags the average reward down substantially.\n",
    "\n",
    "#### 2nd Training\n",
    "- Agent then learned to play a rule-based agent which dominates Negamax\n",
    "- After the agent learned to consistently defeat the rule-based agent, the agent no longer performed very well against Negamax. Perhaps the training sample is too narrow/specialised.\n",
    "\n",
    "<center><img src=\"data/c4_rba_train.png\" width=500 height=500 /></center>\n",
    "\n",
    "#### Later training against ensemble opponent\n",
    "For each game, the opponent ist selected at random. A batch then consists of games against multiple opponents. This I believe is a good approach if you're not applying **experience replay**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Initiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate, make, utils\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = make(\"connectx\", debug=True)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import neural network architectures\n",
    "from NN_architecture import CNN_A1, CNN_A2, CNN_A3, CNN_A4\n",
    "#import Deep Q Learning Model Class \"QModelClass\"\n",
    "from DeepQAgent2 import QModelClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load/initiate an instance of the Deep Q Learning Model Class \"QModelClass\"\n",
    "DQMInstance = QModelClass(model_name =\"allstar_A4_twoface5\", exploration_factor=0.9, alpha= 0.3, early_stopping = True, configuration= env.configuration, model_architecture = CNN_A4)\n",
    "DQMInstance.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the agent with eps-greedy policy\n",
    "def dqn_agent(observation, configuration):\n",
    "    \n",
    "    board_state = DQMInstance.obs_to_board_state(observation)\n",
    "    #epsilon greedy policy\n",
    "    p = rand.uniform(0, 1)\n",
    "\n",
    "    if p < DQMInstance.exp_factor:\n",
    "        #exploitation move\n",
    "        move = DQMInstance.current_optimal_move(board_state)\n",
    "    else:\n",
    "        #exploration move\n",
    "        all_options = DQMInstance.all_options(board_state)\n",
    "        move = int(rand.choice(all_options))\n",
    "            \n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #define weaker (parameters less trained) agent with exploitative policy \n",
    "# DQMInstanceB = QModelClass(model_name =\"allstar_A4_twoface4\", exploration_factor=0.9, alpha= 0.5, early_stopping = True, configuration= env.configuration, model_architecture = CNN_A4)\n",
    "# DQMInstanceB.load_model()\n",
    "\n",
    "# def dqn_exploit_agentB(observation, configuration):\n",
    "#     board_state = DQMInstanceB.obs_to_board_state(observation)\n",
    "#     move = DQMInstanceB.current_optimal_move(board_state)       \n",
    "#     return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define agent with purely exploitative policy \n",
    "def dqn_field_agent(observation, configuration):\n",
    "    board_state = DQMInstance.obs_to_board_state(observation)\n",
    "    move = DQMInstance.current_optimal_move(board_state)  \n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #agent incorporating \"foresight\" (minmax algo on top the deep-q algo)\n",
    "# def dqn_foresight_field_agent(observation, configuration):\n",
    "#     board_state = DQMInstance.obs_to_board_state(observation)\n",
    "#     move = DQMInstance.foresight_optimal_move(board_state)    \n",
    "#     return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import other agents (negamax from kaggle, agent_rule_based found online)\n",
    "from agent_rule_based import rule_based_agent\n",
    "from negamax import negamax_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQMInstance.reset_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs (one epoch here actually refers to a full game)\n",
    "epochs_n = 100000\n",
    "# after how many epochs do we train\n",
    "training_batch = 32\n",
    "# rewards stored in array\n",
    "rewards = []\n",
    "\n",
    "for epoch in range(1, epochs_n + 1):\n",
    "    \n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    \n",
    "    #ensemble training: \n",
    "        #agent randomly plays against negamax, agent_rule_based or itself\n",
    "    j = np.random.randint(3)\n",
    "        #agent randomly starts first or second\n",
    "    l = np.random.randint(2)\n",
    "    if j == 0:\n",
    "        if l == 0:\n",
    "            AGENT = rule_based_agent\n",
    "            trainer = env.train([None, dqn_agent])\n",
    "            print(\"rule_based_agent vs dqn_agent\")\n",
    "        if l == 1:\n",
    "            AGENT = dqn_agent\n",
    "            trainer = env.train([None, rule_based_agent])\n",
    "            print(\"dqn_agent vs rule_based_agent\")\n",
    "    if j == 1:\n",
    "        if l == 0:\n",
    "            AGENT = negamax_agent\n",
    "            trainer = env.train([None, dqn_agent])\n",
    "            print(\"negamax vs dqn_agent\")\n",
    "        if l == 1:\n",
    "            AGENT = dqn_agent\n",
    "            trainer = env.train([None, \"negamax\"])\n",
    "            print(\"dqn_agent vs negamax\")\n",
    "    if j == 2:\n",
    "        AGENT = dqn_agent\n",
    "        trainer = env.train([None, dqn_agent])\n",
    "        print(\"dqn_agent vs dqn_agent\")\n",
    "\n",
    "    observation = trainer.reset()\n",
    "    done = False\n",
    "    move_count = 0\n",
    "    \n",
    "    while not done:\n",
    "        #state\n",
    "        board_state = DQMInstance.obs_to_board_state(env.state[0].observation)\n",
    "        \n",
    "        move_count += 1\n",
    "        if move_count > 1:\n",
    "            #load experience to memory\n",
    "            DQMInstance.load_to_memory(DQMInstance.prev_state, DQMInstance.prev_move, board_state, DQMInstance.reward(env.state[0].status, env.state[0].reward))\n",
    "        if move_count > 2:\n",
    "            #load opponents experience to memory\n",
    "            DQMInstance.load_opponents_experience_to_memory(DQMInstance.past_prev_state, DQMInstance.past_prev_move, DQMInstance.prev_state, DQMInstance.prev_move, \n",
    "                                                DQMInstance.prev_reward_opponent)\n",
    "        #action\n",
    "        my_action = AGENT(env.state[0].observation, env.configuration)\n",
    "        #reward\n",
    "        observation, reward, done, info = trainer.step(my_action)\n",
    "        #cache previous move\n",
    "        DQMInstance.past_prev_state = DQMInstance.prev_state\n",
    "        DQMInstance.past_prev_move = DQMInstance.prev_move\n",
    "        DQMInstance.prev_state = board_state\n",
    "        DQMInstance.prev_move = my_action\n",
    "        DQMInstance.prev_reward = env.state[0].reward\n",
    "        DQMInstance.prev_reward_opponent = env.state[-1].reward\n",
    "    \n",
    "    #final state\n",
    "    board_state_game_finish = DQMInstance.obs_to_board_state(env.state[0].observation)\n",
    "    #load experience to memory\n",
    "    DQMInstance.load_to_memory(DQMInstance.prev_state, DQMInstance.prev_move, board_state_game_finish, DQMInstance.reward(env.state[0].status, env.state[0].reward))\n",
    "    \n",
    "    #load opponents experience to memory\n",
    "    #print(np.count_nonzero(DQMInstance.prev_state-board_state_game_finish))\n",
    "    if np.count_nonzero(DQMInstance.prev_state-board_state_game_finish) <= 1:\n",
    "        DQMInstance.load_opponents_experience_to_memory(DQMInstance.past_prev_state, DQMInstance.past_prev_move, DQMInstance.prev_state, DQMInstance.prev_move, env.state[-1].reward)\n",
    "    if np.count_nonzero(DQMInstance.prev_state-board_state_game_finish) == 2:\n",
    "        DQMInstance.load_opponents_experience_to_memory(DQMInstance.past_prev_state, DQMInstance.past_prev_move, DQMInstance.prev_state, DQMInstance.prev_move, 0)\n",
    "        board_state_1 = DQMInstance.make_state_from_move(DQMInstance.prev_state, DQMInstance.prev_move)\n",
    "        board_state_2 = board_state_game_finish\n",
    "        opp_move = DQMInstance.opponent_move_from_states(board_state_1, board_state_2)\n",
    "        DQMInstance.memory.append([DQMInstance.switch_sides(board_state_1), opp_move, DQMInstance.switch_sides(board_state_2), env.state[-1].reward])\n",
    "\n",
    "    print(f\"Reward: {reward}\")\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    #Offline (Batch) training\n",
    "    if epoch % training_batch == 0:\n",
    "        #fit DQ model to current batch of memory\n",
    "        DQMInstance.learn_batch(DQMInstance.memory)\n",
    "        #save model after every training (training may crash)\n",
    "        DQMInstance.save_model()\n",
    "        #empty memory after every batch\n",
    "        DQMInstance.reset_memory()\n",
    "        #save rewards for this batch of epochs\n",
    "        DQMInstance.save_rewards(rewards)\n",
    "        \n",
    "        rewards = []\n",
    "\n",
    "    \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training results (mean reward per training batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_means = [] \n",
    "batches = []\n",
    "\n",
    "for batch in range(1, 500):\n",
    "    rm = pd.read_pickle(\"data/rewards_list_allstar_A4_twoface5_\" + str(batch) + \".pkl\")\n",
    "    reward_means.append(np.mean(rm))\n",
    "    batches.append(batch)\n",
    "    \n",
    "plt.title(\"Avg. Reward; Pretrained Deep-Q-Agent CNN_A4 vs. RBA & Negamax; Batches of 32\")\n",
    "plt.plot(batches,reward_means)\n",
    "\n",
    "plt.savefig('data/Deep-Q-Agent CNN_A4 vs. RBA & Negamax.jpg',bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_means = [] \n",
    "batches = []\n",
    "\n",
    "for batch in range(1, 1600):\n",
    "    rm = pd.read_pickle(\"data/rewards_list_allstar_A4_twoface_\" + str(batch) + \".pkl\")\n",
    "    reward_means.append(np.mean(rm))\n",
    "    batches.append(batch)\n",
    "    \n",
    "plt.title(\"Avg. Reward; DQA CNN_A4 with two sight vs. ensemble; Batches of 32\")\n",
    "plt.plot(batches,reward_means)\n",
    "\n",
    "plt.savefig('data/DQA CNN_A4 with two sight vs. ensemble.jpg',bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model by having the purely exploitative agent play opponents\n",
    "\n",
    "The Deep Q agent can be seen to dominate both negamax and the rule based agent starting first and second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(\"connectx\", [dqn_field_agent, dqn_foresight_field_agent], num_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(\"connectx\", [dqn_foresight_field_agent, dqn_field_agent], num_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, 1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [-1, 1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [-1, 1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [0, 0],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"connectx\", [dqn_field_agent, \"negamax\"], num_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0],\n",
       " [1, -1],\n",
       " [-1, 1],\n",
       " [1, -1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [0, 0],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [-1, 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"connectx\", [\"negamax\", dqn_field_agent], num_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"connectx\", [dqn_field_agent, rule_based_agent], num_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"connectx\", [rule_based_agent, dqn_field_agent], num_episodes=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

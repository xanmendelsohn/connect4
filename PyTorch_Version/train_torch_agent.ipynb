{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d3f6f1-56cf-44ea-9dcd-9d784b242329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment lux_ai_2022 failed: No module named 'vec_noise'\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import evaluate, make, utils\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = make(\"connectx\", debug=True)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3c2dec-5313-4410-b244-6d98cfa89eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 13:09:19.769703: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /var/lib/cdsw-custom/addons/lib64\n",
      "2023-02-22 13:09:19.769732: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/cdsw/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from NN_architecture import PyCNNik_A4\n",
    "from DeepQAgent_torch import QTorchModelClass, CreateDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a257bcc8-1a7a-46d2-b8d8-65134b3c1335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading modelDQNs/allstar_A4_torch_04.h5\n"
     ]
    }
   ],
   "source": [
    "DQNI = QTorchModelClass(model_name =\"allstar_A4_torch_04\", exploration_factor=0.9, alpha= 0.5, early_stopping = False, configuration= env.configuration, model_architecture = PyCNNik_A4)\n",
    "DQNI.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e82df762-5535-4174-aa8d-f131b1ca7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_agent(observation, configuration):\n",
    "    \n",
    "    board_state = DQNI.obs_to_board_state(observation)\n",
    "    #epsilon greedy policy\n",
    "    p = rand.uniform(0, 1)\n",
    "\n",
    "    if p < DQNI.exp_factor:\n",
    "        #exploitation move\n",
    "        move = DQNI.current_optimal_move(board_state)\n",
    "    else:\n",
    "        #exploration move\n",
    "        all_options = DQNI.all_options(board_state)\n",
    "        move = int(rand.choice(all_options))\n",
    "            \n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a92477b-02a5-46ee-b8b9-ce777bf3c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_field_agent(observation, configuration):\n",
    "    \n",
    "    board_state = DQNI.obs_to_board_state(observation)\n",
    "    \n",
    "    move = DQNI.current_optimal_move(board_state)\n",
    "            \n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35fbe9a5-0f19-4f29-b192-e9059f47dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_rule_based import rule_based_agent\n",
    "from negamax import negamax_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ea5420-2bfd-4b96-ae8e-3f149f70411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQNI.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9082f0-14a1-474d-8ad3-19dfe002edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### training agent \n",
    "########################################################################################################################################################\n",
    "# number of epochs\n",
    "epochs_n = 100000\n",
    "# after how many epochs do we train\n",
    "training_batch = 64\n",
    "rewards = []\n",
    "\n",
    "for epoch in range(1, epochs_n + 1):\n",
    "    \n",
    "    print(f\"Epoch: {epoch}\")\n",
    "\n",
    "    j = np.random.randint(1)\n",
    "    l = np.random.randint(2)\n",
    "    if j == 0:\n",
    "        if l == 0:\n",
    "            AGENT = negamax_agent\n",
    "            trainer = env.train([None, dqn_agent])\n",
    "            print(\"negamax vs dqn_agent\")\n",
    "            r = float(-1)\n",
    "        if l == 1:\n",
    "            AGENT = dqn_agent\n",
    "            trainer = env.train([None, \"negamax\"])\n",
    "            print(\"dqn_agent vs negamax\")\n",
    "            r = float(1)\n",
    "\n",
    "    observation = trainer.reset()\n",
    "    done = False\n",
    "    move_count = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        board_state = DQNI.obs_to_board_state(env.state[0].observation)\n",
    "        \n",
    "        move_count += 1\n",
    "        if move_count > 1:\n",
    "            DQNI.load_to_memory(DQNI.prev_state, DQNI.prev_move, board_state, DQNI.reward(env.state[0].status, env.state[0].reward))\n",
    "        if move_count > 2:\n",
    "            #load opponents experience to memory\n",
    "            DQNI.load_opponents_experience_to_memory(DQNI.past_prev_state, DQNI.past_prev_move, DQNI.prev_state, DQNI.prev_move, \n",
    "                                                DQNI.prev_reward_opponent)\n",
    "\n",
    "        my_action = AGENT(env.state[0].observation, env.configuration)\n",
    "            \n",
    "        observation, reward, done, info = trainer.step(my_action)\n",
    "        \n",
    "        DQNI.past_prev_state = DQNI.prev_state\n",
    "        DQNI.past_prev_move = DQNI.prev_move\n",
    "        DQNI.prev_state = board_state\n",
    "        DQNI.prev_move = my_action\n",
    "        DQNI.prev_reward = env.state[0].reward\n",
    "        DQNI.prev_reward_opponent = env.state[-1].reward\n",
    "        \n",
    "    board_state_game_finish = DQNI.obs_to_board_state(env.state[0].observation)\n",
    "    DQNI.load_to_memory(DQNI.prev_state, DQNI.prev_move, board_state_game_finish, DQNI.reward(env.state[0].status, env.state[0].reward))\n",
    "    \n",
    "    #load opponents experience to memory\n",
    "    #print(np.count_nonzero(DQMInstance.prev_state-board_state_game_finish))\n",
    "    if np.count_nonzero(DQNI.prev_state-board_state_game_finish) <= 1:\n",
    "        DQNI.load_opponents_experience_to_memory(DQNI.past_prev_state, DQNI.past_prev_move, DQNI.prev_state, DQNI.prev_move, env.state[-1].reward)\n",
    "    if np.count_nonzero(DQNI.prev_state-board_state_game_finish) == 2:\n",
    "        DQNI.load_opponents_experience_to_memory(DQNI.past_prev_state, DQNI.past_prev_move, DQNI.prev_state, DQNI.prev_move, 0)\n",
    "        board_state_1 = DQNI.make_state_from_move(DQNI.prev_state, DQNI.prev_move)\n",
    "        board_state_2 = board_state_game_finish\n",
    "        opp_move = DQNI.opponent_move_from_states(board_state_1, board_state_2)\n",
    "        DQNI.memory.append([DQNI.switch_sides(board_state_1), opp_move, DQNI.switch_sides(board_state_2), env.state[-1].reward])\n",
    "\n",
    "    print(f\"Reward for dqn_agent: {reward*r}\")\n",
    "    rewards.append(reward*r)\n",
    "    \n",
    "    # Offline (Batch) training\n",
    "    if epoch % training_batch == 0:\n",
    "        #create torch dataset\n",
    "        training_batch_ds = CreateDatasets(DQNI, DQNI.memory)\n",
    "        # fit\n",
    "        DQNI.learn_batch(training_batch_ds)\n",
    "        # save model after every training (training may crash)\n",
    "        DQNI.save_model()\n",
    "        # empty memory after every batch or let memory accumulate for given number of epochs\n",
    "        if epoch % 320 == 0:\n",
    "            DQNI.reset_memory()\n",
    "        # save rewards for this batch of epochs\n",
    "        DQNI.save_rewards(rewards)\n",
    "        rewards = []\n",
    "\n",
    "    \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab01ff7-d379-4ca2-8681-79e4ef3860b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_means = [] \n",
    "batches = []\n",
    "\n",
    "for batch in range(1, 158):\n",
    "    rm = pd.read_pickle(\"data/rewards_list_allstar_A4_torch_04_\" + str(batch) + \".pkl\")\n",
    "    reward_means.append(np.mean(rm))\n",
    "    batches.append(batch)\n",
    "    \n",
    "plt.title(\"Avg. Reward; Pretrained Deep-Q-Agent CNN_A4 03 vs. Negamax; Batches of 32\")\n",
    "plt.plot(batches,reward_means)\n",
    "\n",
    "plt.savefig('data/Deep-Q-Agent CNN_A4 03 vs. Negamax.jpg',bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94aba4-ccf0-407a-968f-9e99ed54b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\"connectx\", [dqn_field_agent, \"negamax\"], num_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614db08-d5e1-4ec1-ab41-38f4fc8eaef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\"connectx\", [\"negamax\", dqn_field_agent], num_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf74af-00ba-4815-97af-519eaaf79c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
